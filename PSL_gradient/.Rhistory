variable = "geo_longitude") > -105)]
all_iso_ts_green = all_iso_ts_green[which(pullTsVariable(all_iso_ts_green,
variable = "geo_longitude") < 50)]
all_iso_ts_green = all_iso_ts_green[which(pullTsVariable(all_iso_ts_green,
variable = "geo_latitude") > 0)]
all_iso_ts_green = all_iso_ts_green[which(pullTsVariable(all_iso_ts_green,
variable = "geo_latitude") < 85)]
# Define calibration and validation interval
start.cal = 1824
end.cal = 1999
start.val = 1824
end.val = 1872
# PCR interval
start.pcr = 1824
end.pcr = 1970
res.limit = 1.1
# Restrict to records that have data extending to 1860 CE.
#   This threshold could be adjusted based on the desired calibration window.
endYear = 1880
allRecs = matrix(NA, length(all_iso_ts_green), 11) %>%
set_colnames(c("record", "resolution", "duration",
"archive", "lat", "lon", "infMat", "var", "interp",
"season", "start_year"))
for(i in 1:(length(all_iso_ts_green))) {
thisYearVec = na.omit(all_iso_ts_green[[i]]$year)
thisYearVec = subset(thisYearVec, thisYearVec >= start.cal)
if(max(thisYearVec) < endYear) {
next
} else {
allRecs[i, 1] = all_iso_ts_green[[i]]$paleoData_TSid
allRecs[i, 2] = (max(thisYearVec)-min(thisYearVec))/length(thisYearVec)
allRecs[i, 3] = max(thisYearVec)-min(thisYearVec)
allRecs[i, 4] = all_iso_ts_green[[i]]$archiveType
allRecs[i, 5] = all_iso_ts_green[[i]]$geo_latitude
allRecs[i, 6] = all_iso_ts_green[[i]]$geo_longitude
allRecs[i, 7] = all_iso_ts_green[[i]]$paleoData_inferredMaterial
allRecs[i, 8] = all_iso_ts_green[[i]]$paleoData_variableName
allRecs[i, 9] = all_iso_ts_green[[i]]$isotopeInterpretation1_variableGroup
allRecs[i, 10] = if(!is.null(all_iso_ts_green[[i]]$isotopeInterpretation1_seasonality)){
all_iso_ts_green[[i]]$isotopeInterpretation1_seasonality} else {NA}
allRecs[i, 11] = min(thisYearVec)
}
}
allRecs = as.data.frame(allRecs) %>%
mutate_at(c("resolution", "lat", "lon", "duration"), as.numeric)
# Filter for annual resolution
is_annual = which(allRecs$resolution <= res.limit)
ann_recs_df = allRecs[is_annual, ]
ann_iso_ts = all_iso_ts_green[is_annual]
#### Import and plot Vinther DJF and Vinther Annual ####
if (Sys.info()['sysname'] == "Darwin"){
setwd("~/Box Sync/Konecky Lab/User Storage/Andrew Flaim/Rcode/Data") # MacOS
} else if (Sys.info()['login'] == "andre"){
setwd("C:/Users/andre/Box Sync/Konecky Lab/User Storage/Andrew Flaim/Rcode/Data") # Laptop
} else{
setwd("D:/Box/Box_Sync/Konecky Lab/User Storage/Andrew Flaim/Rcode/Data") # Desktop
}
base.djf = na.omit(read.csv("Vinther2003_NAOindex_1821-1999monthly.csv", head = T))
#
#### Import Greenland Iso2k PC1 ####
# See "Iso2k_NAO_Greenland.R" for source
if (Sys.info()['sysname'] == "Darwin"){
setwd("~/Box Sync/Konecky Lab/User Storage/Andrew Flaim/Rcode/Data") # MacOS
} else if (Sys.info()['login'] == "andre"){
setwd("C:/Users/andre/Box Sync/Konecky Lab/User Storage/Andrew Flaim/Rcode/Data") # Laptop
} else{
setwd("D:/Box/Box_Sync/Konecky Lab/User Storage/Andrew Flaim/Rcode/Data") # Desktop
}
greenPC1 = read.csv("greenPC1.csv", head = T)
#
#### Bin data ####
binvec =  seq(from = 1000, to = 1999, by = 1)
binYears = rowMeans(cbind(binvec[-1], binvec[-length(binvec)]))
binned_ann_recs = matrix(NA, length(binYears), length(ann_iso_ts))
for(i in 1:(ncol(binned_ann_recs))) {
thisrec = na.omit(data.frame(year = ann_iso_ts[[i]]$year, val = ann_iso_ts[[i]]$paleoData_values))
binned_ann_recs[,i] = geoChronR::bin(thisrec$year, thisrec$val, binvec)$y
}
# Bin the base TS
base.bin = geoChronR::bin(base.djf$Year, base.djf$DJF, binvec)
binned_ann_recs_iso2k = binned_ann_recs
# Subset for data overlap interval
binned_ann_recs_cal = binned_ann_recs[which(binYears >= start.pcr & binYears <= end.pcr), ]
base.bin.cal = base.bin[which(binYears >= start.pcr & binYears <= end.pcr), ]
#### Filter out any records missing more data than the threshold ####
filter.thresh = (6.6/10)
keep_index = list()
counter = 1
for(i in 1:(ncol(binned_ann_recs_cal))) {
if(length(na.omit(binned_ann_recs_cal[ ,i]))/length(binned_ann_recs_cal[ ,i]) >= filter.thresh) {
keep_index[[counter]] = i
counter = counter + 1
}
}
keep_index = unlist(keep_index)
ann_keep = binned_ann_recs[ ,keep_index]
ann_keep_cal = binned_ann_recs_cal[ ,keep_index]
ann_keep_df = ann_recs_df[keep_index, ]
#
#   setwd("~/Box Sync/Konecky Lab/User Storage/Andrew Flaim/Rcode/Data") # MacOS
# } else if (Sys.info()['login'] == "andre"){
#   setwd("C:/Users/andre/Box Sync/Konecky Lab/User Storage/Andrew Flaim/Rcode/Data") # Laptop
# } else{
#   setwd("D:/Box/Box_Sync/Konecky Lab/User Storage/Andrew Flaim/Rcode/Data") # Desktop
# }
# write.csv(ann_keep_df, "1880-1990_Atlantic_66%.csv")
#
#### Map Atlantic annual 66% coverage ####
# Define archive point types
shapes = c("Coral" = 15, "GlacierIce" = 20, "Sclerosponge" = 15, "Wood" = 18,
"LakeSediment" = 17, "Speleothem" = 15, "MolluskShells" = 15,
"MarineSediment" = 17)
ggplot() +
geom_polygon(data = wrld_simpl, aes(x = long, y = lat, group = group),
fill = "grey", colour = "black", alpha = 0.2) +
geom_point(data = ann_keep_df, mapping = aes(x = lon, y = lat,
shape = archive), color = "black", size = 7) +
geom_point(data = ann_keep_df, mapping = aes(x = lon, y = lat,
shape = archive), color = "grey", size = 5) +
scale_shape_manual(values = shapes) +
# Removes Axes and labels
scale_x_continuous(breaks = NULL) +
xlab("") +
ylab("") +
# Change theme to remove axes and ticks
theme(panel.background = element_blank(),
axis.ticks=element_blank(),
panel.border = element_blank(),
panel.grid.major = element_blank())
ggplot() +
geom_polygon(data = wrld_simpl, aes(x = long, y = lat, group = group),
fill = "grey", colour = "black", alpha = 0.2) +
geom_point(data = ann_keep_df, mapping = aes(x = lon, y = lat,
shape = archive), color = "black", size = 7) +
geom_point(data = ann_keep_df, mapping = aes(x = lon, y = lat,
shape = archive), color = "blue", size = 5) +
scale_shape_manual(values = shapes) +
# Removes Axes and labels
scale_x_continuous(breaks = NULL) +
xlab("") +
ylab("") +
# Change theme to remove axes and ticks
theme(panel.background = element_blank(),
axis.ticks=element_blank(),
panel.border = element_blank(),
panel.grid.major = element_blank())
#### 1824-1970 EXCLUDE GREENLAND SUBSET ####
library(magrittr)
library(tidyverse)
library(geoChronR)
library(lubridate)
library(rgdal)
library(progress)
library(rgeos)
library(sp)
#
#### Extracting Atlantic Annual Series ####
data("wrld_simpl", package = "maptools")
# Load and extract Iso2k
setwd("C:/Users/andre/Box Sync/Konecky Lab/User Storage/Andrew Flaim/Rcode/Iso2k") # Laptop
load("iso2k1_0_0.RData")
# Remove extraneous objects
rm(D, TS)
# Apply geoChronR function to extrat primary TS
all_iso_ts = sTS[which(pullTsVariable(sTS,
variable = "paleoData_iso2kPrimaryTimeseries") == "TRUE")]
# Extract Atlantic series
all_iso_ts = all_iso_ts[which(pullTsVariable(all_iso_ts,
variable = "geo_longitude") > -179)]
all_iso_ts = all_iso_ts[which(pullTsVariable(all_iso_ts,
variable = "geo_longitude") < 50)]
all_iso_ts = all_iso_ts[which(pullTsVariable(all_iso_ts,
variable = "geo_latitude") < 60)]
all_iso_ts = all_iso_ts[which(pullTsVariable(all_iso_ts,
variable = "geo_latitude") > 0)]
# Define calibration interval
start.cal = 1824
end.cal = 1970
res.limit = 1.1
# Restrict to records that have data extending to 1860 CE.
#   This threshold could be adjusted based on the desired calibration window.
endYear = 1880
allRecs = matrix(NA, length(all_iso_ts), 11) %>%
set_colnames(c("record", "resolution", "duration",
"archive", "lat", "lon", "infMat", "var", "interp",
"season", "start_year"))
for(i in 1:(length(all_iso_ts))) {
thisYearVec = na.omit(all_iso_ts[[i]]$year)
thisYearVec = subset(thisYearVec, thisYearVec >= start.cal)
if(max(thisYearVec) < endYear) {
next
} else {
allRecs[i, 1] = all_iso_ts[[i]]$paleoData_TSid
allRecs[i, 2] = (max(thisYearVec)-min(thisYearVec))/length(thisYearVec)
allRecs[i, 3] = max(thisYearVec)-min(thisYearVec)
allRecs[i, 4] = all_iso_ts[[i]]$archiveType
allRecs[i, 5] = all_iso_ts[[i]]$geo_latitude
allRecs[i, 6] = all_iso_ts[[i]]$geo_longitude
allRecs[i, 7] = all_iso_ts[[i]]$paleoData_inferredMaterial
allRecs[i, 8] = all_iso_ts[[i]]$paleoData_variableName
allRecs[i, 9] = all_iso_ts[[i]]$isotopeInterpretation1_variableGroup
allRecs[i, 10] = if(!is.null(all_iso_ts[[i]]$isotopeInterpretation1_seasonality)){
all_iso_ts[[i]]$isotopeInterpretation1_seasonality} else {NA}
allRecs[i, 11] = min(thisYearVec)
}
}
allRecs = as.data.frame(allRecs) %>%
mutate_at(c("resolution", "lat", "lon", "duration"), as.numeric)
# Filter for annual resolution
is_annual = which(allRecs$resolution <= res.limit)
ann_recs_df = allRecs[is_annual, ]
ann_iso_ts = all_iso_ts[is_annual]
#
#### Import CDU NOA DJF data ####
setwd("C:/Users/andre/Box Sync/Konecky Lab/User Storage/Andrew Flaim/Rcode/Data") # Laptop
base.djf = read.csv("CRUData_NAO_DJF.csv", header = T)
base.djf = na.omit(base.djf)
base.djf = base.djf[, c(1, 5)]
#
#### Bin data ####
binvec =  seq(from = 1000, to = 1999, by = 1)
binYears = rowMeans(cbind(binvec[-1], binvec[-length(binvec)]))
binned_ann_recs = matrix(NA, length(binYears), length(ann_iso_ts))
for(i in 1:(ncol(binned_ann_recs))) {
thisrec = na.omit(data.frame(year = ann_iso_ts[[i]]$year, val = ann_iso_ts[[i]]$paleoData_values))
binned_ann_recs[,i] = geoChronR::bin(thisrec$year, thisrec$val, binvec)$y
}
# Bin the base TS
base.bin = geoChronR::bin(base.djf$year, base.djf$djf, binvec)
binned_ann_recs_iso2k = binned_ann_recs
# Subset for calibration interval
binned_ann_recs_cal = binned_ann_recs[which(binYears >= start.cal & binYears <= end.cal), ]
base.bin.cal = base.bin[which(binYears >= start.cal & binYears <= end.cal), ]
################################### COVERAGE = 90% #############################
#### Filter out any records missing more data than the threshold ####
filter.thresh = (6.6/10)
keep_index = list()
counter = 1
for(i in 1:(ncol(binned_ann_recs_cal))) {
if(length(na.omit(binned_ann_recs_cal[ ,i]))/length(binned_ann_recs_cal[ ,i]) >= filter.thresh) {
keep_index[[counter]] = i
counter = counter + 1
}
}
keep_index = unlist(keep_index)
ann_keep = binned_ann_recs[ ,keep_index]
ann_keep_cal = binned_ann_recs_cal[ ,keep_index]
ann_keep_df = ann_recs_df[keep_index, ]
#
#### Map records with annual 66% coverage ####
# Define archive point types
shapes = c("Coral" = 15, "GlacierIce" = 20, "Sclerosponge" = 19, "Wood" = 18,
"LakeSediment" = 17, "Speleothem" = 15, "MolluskShells" = 19,
"MarineSediment" = 17)
ggplot() +
geom_polygon(data = wrld_simpl, aes(x = long, y = lat, group = group),
fill = "grey", colour = "black", alpha = 0.2) +
geom_point(data = ann_keep_df, mapping = aes(x = lon, y = lat,
shape = archive), color = "black", size = 7) +
geom_point(data = ann_keep_df, mapping = aes(x = lon, y = lat,
shape = archive), color = "blue", size = 5) +
scale_shape_manual(values = shapes) +
# Removes Axes and labels
scale_x_continuous(breaks = NULL) +
xlab("") +
ylab("") +
# Change theme to remove axes and ticks
theme(panel.background = element_blank(),
axis.ticks=element_blank(),
panel.border = element_blank(),
panel.grid.major = element_blank())
#
#### 1824-1970 EXCLUDE GREENLAND SUBSET ####
library(magrittr)
library(tidyverse)
library(geoChronR)
library(lubridate)
library(rgdal)
library(progress)
library(rgeos)
library(sp)
#
#### Extracting Atlantic Annual Series ####
data("wrld_simpl", package = "maptools")
# Load and extract Iso2k
setwd("C:/Users/andre/Box Sync/Konecky Lab/User Storage/Andrew Flaim/Rcode/Iso2k") # Laptop
load("iso2k1_0_0.RData")
# Remove extraneous objects
rm(D, TS)
# Apply geoChronR function to extrat primary TS
all_iso_ts = sTS[which(pullTsVariable(sTS,
variable = "paleoData_iso2kPrimaryTimeseries") == "TRUE")]
# Extract Atlantic series
all_iso_ts = all_iso_ts[which(pullTsVariable(all_iso_ts,
variable = "geo_longitude") > -105)]
all_iso_ts = all_iso_ts[which(pullTsVariable(all_iso_ts,
variable = "geo_longitude") < 50)]
all_iso_ts = all_iso_ts[which(pullTsVariable(all_iso_ts,
variable = "geo_latitude") < 60)]
all_iso_ts = all_iso_ts[which(pullTsVariable(all_iso_ts,
variable = "geo_latitude") > 0)]
# Define calibration interval
start.cal = 1824
end.cal = 1970
res.limit = 1.1
# Restrict to records that have data extending to 1860 CE.
#   This threshold could be adjusted based on the desired calibration window.
endYear = 1880
allRecs = matrix(NA, length(all_iso_ts), 11) %>%
set_colnames(c("record", "resolution", "duration",
"archive", "lat", "lon", "infMat", "var", "interp",
"season", "start_year"))
for(i in 1:(length(all_iso_ts))) {
thisYearVec = na.omit(all_iso_ts[[i]]$year)
thisYearVec = subset(thisYearVec, thisYearVec >= start.cal)
if(max(thisYearVec) < endYear) {
next
} else {
allRecs[i, 1] = all_iso_ts[[i]]$paleoData_TSid
allRecs[i, 2] = (max(thisYearVec)-min(thisYearVec))/length(thisYearVec)
allRecs[i, 3] = max(thisYearVec)-min(thisYearVec)
allRecs[i, 4] = all_iso_ts[[i]]$archiveType
allRecs[i, 5] = all_iso_ts[[i]]$geo_latitude
allRecs[i, 6] = all_iso_ts[[i]]$geo_longitude
allRecs[i, 7] = all_iso_ts[[i]]$paleoData_inferredMaterial
allRecs[i, 8] = all_iso_ts[[i]]$paleoData_variableName
allRecs[i, 9] = all_iso_ts[[i]]$isotopeInterpretation1_variableGroup
allRecs[i, 10] = if(!is.null(all_iso_ts[[i]]$isotopeInterpretation1_seasonality)){
all_iso_ts[[i]]$isotopeInterpretation1_seasonality} else {NA}
allRecs[i, 11] = min(thisYearVec)
}
}
allRecs = as.data.frame(allRecs) %>%
mutate_at(c("resolution", "lat", "lon", "duration"), as.numeric)
# Filter for annual resolution
is_annual = which(allRecs$resolution <= res.limit)
ann_recs_df = allRecs[is_annual, ]
ann_iso_ts = all_iso_ts[is_annual]
#
#### Import CDU NOA DJF data ####
setwd("C:/Users/andre/Box Sync/Konecky Lab/User Storage/Andrew Flaim/Rcode/Data") # Laptop
base.djf = read.csv("CRUData_NAO_DJF.csv", header = T)
base.djf = na.omit(base.djf)
base.djf = base.djf[, c(1, 5)]
#
#### Bin data ####
binvec =  seq(from = 1000, to = 1999, by = 1)
binYears = rowMeans(cbind(binvec[-1], binvec[-length(binvec)]))
binned_ann_recs = matrix(NA, length(binYears), length(ann_iso_ts))
for(i in 1:(ncol(binned_ann_recs))) {
thisrec = na.omit(data.frame(year = ann_iso_ts[[i]]$year, val = ann_iso_ts[[i]]$paleoData_values))
binned_ann_recs[,i] = geoChronR::bin(thisrec$year, thisrec$val, binvec)$y
}
# Bin the base TS
base.bin = geoChronR::bin(base.djf$year, base.djf$djf, binvec)
binned_ann_recs_iso2k = binned_ann_recs
# Subset for calibration interval
binned_ann_recs_cal = binned_ann_recs[which(binYears >= start.cal & binYears <= end.cal), ]
base.bin.cal = base.bin[which(binYears >= start.cal & binYears <= end.cal), ]
################################### COVERAGE = 90% #############################
#### Filter out any records missing more data than the threshold ####
filter.thresh = (6.6/10)
keep_index = list()
counter = 1
for(i in 1:(ncol(binned_ann_recs_cal))) {
if(length(na.omit(binned_ann_recs_cal[ ,i]))/length(binned_ann_recs_cal[ ,i]) >= filter.thresh) {
keep_index[[counter]] = i
counter = counter + 1
}
}
keep_index = unlist(keep_index)
ann_keep = binned_ann_recs[ ,keep_index]
ann_keep_cal = binned_ann_recs_cal[ ,keep_index]
ann_keep_df = ann_recs_df[keep_index, ]
#
#### Map records with annual 66% coverage ####
# Define archive point types
shapes = c("Coral" = 15, "GlacierIce" = 20, "Sclerosponge" = 19, "Wood" = 18,
"LakeSediment" = 17, "Speleothem" = 15, "MolluskShells" = 19,
"MarineSediment" = 17)
ggplot() +
geom_polygon(data = wrld_simpl, aes(x = long, y = lat, group = group),
fill = "grey", colour = "black", alpha = 0.2) +
geom_point(data = ann_keep_df, mapping = aes(x = lon, y = lat,
shape = archive), color = "black", size = 7) +
geom_point(data = ann_keep_df, mapping = aes(x = lon, y = lat,
shape = archive), color = "blue", size = 5) +
scale_shape_manual(values = shapes) +
# Removes Axes and labels
scale_x_continuous(breaks = NULL) +
xlab("") +
ylab("") +
# Change theme to remove axes and ticks
theme(panel.background = element_blank(),
axis.ticks=element_blank(),
panel.border = element_blank(),
panel.grid.major = element_blank())
#
setwd("~/GitHub/iso-project/PSL_gradient")
f1 = read.csv("PSL_gradien_F1.csv", head = T)
f1 = read.csv("PSL_gradient_F1.csv", head = T)
View(f1)
plot(f1, type = "l")
f1[1,1]
f1[1872,1]
time = seq(from = 1850, to = 2006, by = 12)
time = seq(from = 1850, to = 2006, by = 1/12)
time = time[-1,]
time = time[-1]
head(time)
time = seq(from = 1850, to = 2006, by = 1/12)
head(time)
time = time[-1]
plot(time, f1$PSL, type = "l")
f2 = read.csv("PSL_gradient_F2.csv", head = T)
f3 = read.csv("PSL_gradient_F3.csv", head = T)
plot(time, f1$PSL, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4))
par(new = T)
plot(time, f2$PSL, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4))
par(new = T)
plot(time, f3$PSL, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4))
plot(time, f1$PSL, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "grey")
par(new = T)
plot(time, f2$PSL, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "grey")
par(new = T)
plot(time, f3$PSL, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "grey")
f1 = read.csv("PSL_gradient_F1.csv", head = T)[,2]
mean = mean(f1, f2, f3)
full = cbind(f1, f2, f3)
View(full)
f1 = read.csv("PSL_gradient_F1.csv", head = T)[,2]
f2 = read.csv("PSL_gradient_F2.csv", head = T)[,2]
f3 = read.csv("PSL_gradient_F3.csv", head = T)[,2]
mean = mean(f1, f2, f3)
full = cbind(f1, f2, f3)
mean = rowMeans(full)
plot(time, mean, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "black")
plot(time, f1, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "grey")
par(new = T)
plot(time, f2, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "grey")
par(new = T)
plot(time, f3, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "grey")
par(new = T)
plot(time, mean, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "black")
axis(1)
axis(2)
axis(2, labels = "Year (CE)")
plot(time, f1, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "grey")
par(new = T)
plot(time, f2, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "grey")
par(new = T)
plot(time, f3, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "grey")
par(new = T)
plot(time, mean, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "black", main = "iCESM full forcing ensemble MPSL index")
axis(1)
axis(2)
#### GHG forcing ####
ghg1 = read.csv("PSL_gradient_GHG.csv", head = T)[,2]
plot(time, ghg1, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "black", main = "iCESM GHG MPSL index")
axis(1)
axis(2)
#### Full & GHG ####
plot(time, mean, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "black", main = "iCESM full forcing & GHG MPSL index")
par(new = T)
plot(time, ghg1, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "green", main = "iCESM GHG MPSL index")
#### Full & GHG ####
plot(time, mean, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "black", main = "iCESM full forcing & GHG MPSL index")
par(new = T)
plot(time, ghg1, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "forest green", main = "")
axis(1)
axis(2)
#### Full & GHG ####
plot(time, mean, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "black", main = "iCESM full forcing & GHG MPSL index", lwd = 2)
par(new = T)
plot(time, ghg1, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "forest green", main = "")
axis(1)
axis(2)
plot(time, f1, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "grey")
par(new = T)
plot(time, f2, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "grey")
par(new = T)
plot(time, f3, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "grey")
par(new = T)
plot(time, mean, type = "l", frame.plot = F, axes = F, xlab = "", ylab = "",
ylim = c(-4, 4), col = "black", main = "iCESM full forcing ensemble MPSL index")
axis(1)
axis(2)
